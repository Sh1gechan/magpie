Instruction Generation Manager. Arguments: Namespace(model_path='google/gemma-2-27b-it', temperature=1.0, top_p=1.0, n=200, repeat=None, total_prompts=100000, max_tokens=2048, max_model_len=4096, early_stopping=True, system_prompt=False, sanitize=False, logits_processor=False, control_tasks='code', shuffle=True, skip_special_tokens=True, checkpoint_every=100, engine='vllm', device='0,1,2,3', dtype='bfloat16', tensor_parallel_size=4, gpu_memory_utilization=0.95, swap_space=2.0, output_folder='data', job_name=None, timestamp=1727008581, seed=None)
WARNING 09-22 21:36:21 utils.py:721] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).
INFO 09-22 21:36:21 config.py:813] Defaulting to use mp for distributed inference
INFO 09-22 21:36:21 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='google/gemma-2-27b-it', speculative_config=None, tokenizer='google/gemma-2-27b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1727008581, served_model_name=google/gemma-2-27b-it, use_v2_block_manager=False, enable_prefix_caching=True)
WARNING 09-22 21:36:23 multiproc_gpu_executor.py:59] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 09-22 21:36:23 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=2259474)[0;0m INFO 09-22 21:36:24 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2259472)[0;0m INFO 09-22 21:36:24 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2259473)[0;0m INFO 09-22 21:36:24 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2259474)[0;0m INFO 09-22 21:36:25 utils.py:975] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2259472)[0;0m INFO 09-22 21:36:25 utils.py:975] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2259473)[0;0m INFO 09-22 21:36:25 utils.py:975] Found nccl from library libnccl.so.2
INFO 09-22 21:36:25 utils.py:975] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2259474)[0;0m INFO 09-22 21:36:25 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=2259472)[0;0m INFO 09-22 21:36:25 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=2259473)[0;0m INFO 09-22 21:36:25 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 09-22 21:36:25 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=2259474)[0;0m WARNING 09-22 21:36:26 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2259473)[0;0m WARNING 09-22 21:36:26 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=2259472)[0;0m WARNING 09-22 21:36:26 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-22 21:36:26 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-22 21:36:26 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f9f8c875ee0>, local_subscribe_port=44891, remote_subscribe_port=None)
[1;36m(VllmWorkerProcess pid=2259472)[0;0m INFO 09-22 21:36:26 model_runner.py:879] Starting to load model google/gemma-2-27b-it...
INFO 09-22 21:36:26 model_runner.py:879] Starting to load model google/gemma-2-27b-it...
[1;36m(VllmWorkerProcess pid=2259473)[0;0m INFO 09-22 21:36:26 model_runner.py:879] Starting to load model google/gemma-2-27b-it...
[1;36m(VllmWorkerProcess pid=2259474)[0;0m INFO 09-22 21:36:26 model_runner.py:879] Starting to load model google/gemma-2-27b-it...
[1;36m(VllmWorkerProcess pid=2259474)[0;0m INFO 09-22 21:36:27 weight_utils.py:236] Using model weights format ['*.safetensors']
INFO 09-22 21:36:27 weight_utils.py:236] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2259472)[0;0m INFO 09-22 21:36:28 weight_utils.py:236] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2259473)[0;0m INFO 09-22 21:36:28 weight_utils.py:236] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2259472)[0;0m INFO 09-22 21:45:43 model_runner.py:890] Loading model weights took 12.7267 GB
[1;36m(VllmWorkerProcess pid=2259474)[0;0m INFO 09-22 21:45:43 model_runner.py:890] Loading model weights took 12.7267 GB
[1;36m(VllmWorkerProcess pid=2259473)[0;0m INFO 09-22 21:45:44 model_runner.py:890] Loading model weights took 12.7267 GB
INFO 09-22 21:45:44 model_runner.py:890] Loading model weights took 12.7267 GB
INFO 09-22 21:46:06 distributed_gpu_executor.py:56] # GPU blocks: 41913, # CPU blocks: 1424
INFO 09-22 21:46:08 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=2259473)[0;0m INFO 09-22 21:46:08 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 09-22 21:46:08 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2259473)[0;0m INFO 09-22 21:46:08 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2259472)[0;0m INFO 09-22 21:46:08 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=2259472)[0;0m INFO 09-22 21:46:08 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2259474)[0;0m INFO 09-22 21:46:08 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=2259474)[0;0m INFO 09-22 21:46:08 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2259473)[0;0m INFO 09-22 21:46:52 model_runner.py:1300] Graph capturing finished in 44 secs.
[1;36m(VllmWorkerProcess pid=2259472)[0;0m INFO 09-22 21:46:52 model_runner.py:1300] Graph capturing finished in 44 secs.
INFO 09-22 21:46:52 model_runner.py:1300] Graph capturing finished in 44 secs.
[1;36m(VllmWorkerProcess pid=2259474)[0;0m INFO 09-22 21:46:52 model_runner.py:1300] Graph capturing finished in 44 secs.
INFO 09-22 21:46:52 block_manager_v1.py:263] Automatic prefix caching is enabled.
Control task: {args.control_tasks}
Pre-query template: <bos><start_of_turn>user

You are an AI assistant designed to provide helpful, step-by-step guidance on coding problems. You can generate a wide variety of coding problems. You can generate a wide variety of problems, including algorithmic and data structures, informational mathematics, real-world problems using programming language frameworks, etc. Generate questions that would appear on a coding exam or in the real world. Êó•Êú¨Ë™û„ÅßÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ


Stop tokens: ['<eos>', '<bos>', '<end_of_turn>', 'assistant', '\n']
Stop token ids: [1, 2, 107]
Checkpoint saved. Total prompts: 200
Checkpoint saved. Total prompts: 20200
Checkpoint saved. Total prompts: 40200
Checkpoint saved. Total prompts: 60200
Checkpoint saved. Total prompts: 80200
Instruction generated from google/gemma-2-27b-it. Total prompts: 100000
ERROR 09-22 22:02:07 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 2259472 died, exit code: -15
INFO 09-22 22:02:07 multiproc_worker_utils.py:123] Killing local vLLM worker processes
