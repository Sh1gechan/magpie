Instruction Generation Manager. Arguments: Namespace(model_path='mistralai/Mixtral-8x22B-Instruct-v0.1', temperature=1.0, top_p=1.0, n=200, repeat=None, total_prompts=50, max_tokens=2048, max_model_len=4096, early_stopping=True, system_prompt=False, sanitize=False, logits_processor=False, control_tasks='code', shuffle=True, skip_special_tokens=True, checkpoint_every=100, engine='vllm', device='0,1,2,3,4,5,6,7', dtype='bfloat16', tensor_parallel_size=8, gpu_memory_utilization=0.95, swap_space=2.0, output_folder='data', job_name=None, timestamp=1726226005, seed=None)
INFO 09-13 20:13:26 config.py:813] Defaulting to use mp for distributed inference
INFO 09-13 20:13:26 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='mistralai/Mixtral-8x22B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x22B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1726226005, served_model_name=mistralai/Mixtral-8x22B-Instruct-v0.1, use_v2_block_manager=False, enable_prefix_caching=True)
WARNING 09-13 20:13:27 multiproc_gpu_executor.py:59] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 09-13 20:13:27 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=380465)[0;0m INFO 09-13 20:13:27 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=380468)[0;0m INFO 09-13 20:13:27 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=380463)[0;0m INFO 09-13 20:13:27 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=380467)[0;0m INFO 09-13 20:13:27 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=380466)[0;0m INFO 09-13 20:13:27 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=380464)[0;0m INFO 09-13 20:13:27 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=380462)[0;0m INFO 09-13 20:13:27 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=380468)[0;0m INFO 09-13 20:13:31 utils.py:975] Found nccl from library libnccl.so.2
INFO 09-13 20:13:31 utils.py:975] Found nccl from library libnccl.so.2
INFO 09-13 20:13:31 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=380463)[0;0m INFO 09-13 20:13:31 utils.py:975] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=380468)[0;0m INFO 09-13 20:13:31 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=380464)[0;0m INFO 09-13 20:13:31 utils.py:975] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=380463)[0;0m INFO 09-13 20:13:31 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=380466)[0;0m INFO 09-13 20:13:31 utils.py:975] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=380464)[0;0m INFO 09-13 20:13:31 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=380466)[0;0m INFO 09-13 20:13:31 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=380465)[0;0m INFO 09-13 20:13:31 utils.py:975] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=380465)[0;0m INFO 09-13 20:13:31 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=380462)[0;0m INFO 09-13 20:13:31 utils.py:975] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=380462)[0;0m INFO 09-13 20:13:31 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=380467)[0;0m INFO 09-13 20:13:31 utils.py:975] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=380467)[0;0m INFO 09-13 20:13:31 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=380465)[0;0m WARNING 09-13 20:13:34 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=380466)[0;0m WARNING 09-13 20:13:34 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=380462)[0;0m WARNING 09-13 20:13:34 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=380464)[0;0m WARNING 09-13 20:13:34 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=380467)[0;0m WARNING 09-13 20:13:34 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=380468)[0;0m WARNING 09-13 20:13:34 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=380463)[0;0m WARNING 09-13 20:13:34 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-13 20:13:34 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-13 20:13:34 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f4f7c2fae50>, local_subscribe_port=41677, remote_subscribe_port=None)
INFO 09-13 20:13:34 model_runner.py:879] Starting to load model mistralai/Mixtral-8x22B-Instruct-v0.1...
[1;36m(VllmWorkerProcess pid=380463)[0;0m INFO 09-13 20:13:34 model_runner.py:879] Starting to load model mistralai/Mixtral-8x22B-Instruct-v0.1...
[1;36m(VllmWorkerProcess pid=380465)[0;0m INFO 09-13 20:13:34 model_runner.py:879] Starting to load model mistralai/Mixtral-8x22B-Instruct-v0.1...
[1;36m(VllmWorkerProcess pid=380466)[0;0m INFO 09-13 20:13:34 model_runner.py:879] Starting to load model mistralai/Mixtral-8x22B-Instruct-v0.1...
[1;36m(VllmWorkerProcess pid=380462)[0;0m INFO 09-13 20:13:34 model_runner.py:879] Starting to load model mistralai/Mixtral-8x22B-Instruct-v0.1...
[1;36m(VllmWorkerProcess pid=380467)[0;0m INFO 09-13 20:13:34 model_runner.py:879] Starting to load model mistralai/Mixtral-8x22B-Instruct-v0.1...
[1;36m(VllmWorkerProcess pid=380464)[0;0m INFO 09-13 20:13:34 model_runner.py:879] Starting to load model mistralai/Mixtral-8x22B-Instruct-v0.1...
[1;36m(VllmWorkerProcess pid=380468)[0;0m INFO 09-13 20:13:34 model_runner.py:879] Starting to load model mistralai/Mixtral-8x22B-Instruct-v0.1...
[1;36m(VllmWorkerProcess pid=380466)[0;0m INFO 09-13 20:13:35 weight_utils.py:236] Using model weights format ['*.safetensors']
INFO 09-13 20:13:35 weight_utils.py:236] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=380464)[0;0m INFO 09-13 20:13:35 weight_utils.py:236] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=380467)[0;0m INFO 09-13 20:13:35 weight_utils.py:236] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=380462)[0;0m INFO 09-13 20:13:35 weight_utils.py:236] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=380465)[0;0m INFO 09-13 20:13:35 weight_utils.py:236] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=380463)[0;0m INFO 09-13 20:13:35 weight_utils.py:236] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=380468)[0;0m INFO 09-13 20:13:35 weight_utils.py:236] Using model weights format ['*.safetensors']
INFO 09-13 20:55:35 model_runner.py:890] Loading model weights took 32.7642 GB
[1;36m(VllmWorkerProcess pid=380467)[0;0m INFO 09-13 20:55:35 model_runner.py:890] Loading model weights took 32.7642 GB
[1;36m(VllmWorkerProcess pid=380468)[0;0m INFO 09-13 20:55:35 model_runner.py:890] Loading model weights took 32.7642 GB
[1;36m(VllmWorkerProcess pid=380465)[0;0m INFO 09-13 20:55:35 model_runner.py:890] Loading model weights took 32.7642 GB
[1;36m(VllmWorkerProcess pid=380463)[0;0m INFO 09-13 20:55:35 model_runner.py:890] Loading model weights took 32.7642 GB
[1;36m(VllmWorkerProcess pid=380464)[0;0m INFO 09-13 20:55:35 model_runner.py:890] Loading model weights took 32.7642 GB
[1;36m(VllmWorkerProcess pid=380466)[0;0m INFO 09-13 20:55:35 model_runner.py:890] Loading model weights took 32.7642 GB
[1;36m(VllmWorkerProcess pid=380462)[0;0m INFO 09-13 20:55:35 model_runner.py:890] Loading model weights took 32.7642 GB
INFO 09-13 20:55:46 distributed_gpu_executor.py:56] # GPU blocks: 96713, # CPU blocks: 4681
[1;36m(VllmWorkerProcess pid=380465)[0;0m INFO 09-13 20:55:48 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=380465)[0;0m INFO 09-13 20:55:48 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 09-13 20:55:48 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 09-13 20:55:48 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=380462)[0;0m INFO 09-13 20:55:48 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=380462)[0;0m INFO 09-13 20:55:48 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=380468)[0;0m INFO 09-13 20:55:48 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=380468)[0;0m INFO 09-13 20:55:48 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=380467)[0;0m INFO 09-13 20:55:48 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=380467)[0;0m INFO 09-13 20:55:48 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=380463)[0;0m INFO 09-13 20:55:48 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=380463)[0;0m INFO 09-13 20:55:48 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=380464)[0;0m INFO 09-13 20:55:48 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=380464)[0;0m INFO 09-13 20:55:48 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=380466)[0;0m INFO 09-13 20:55:48 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=380466)[0;0m INFO 09-13 20:55:48 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=380468)[0;0m INFO 09-13 20:56:10 model_runner.py:1300] Graph capturing finished in 22 secs.
[1;36m(VllmWorkerProcess pid=380465)[0;0m INFO 09-13 20:56:10 model_runner.py:1300] Graph capturing finished in 22 secs.
[1;36m(VllmWorkerProcess pid=380462)[0;0m INFO 09-13 20:56:10 model_runner.py:1300] Graph capturing finished in 22 secs.
[1;36m(VllmWorkerProcess pid=380466)[0;0m INFO 09-13 20:56:10 model_runner.py:1300] Graph capturing finished in 22 secs.
INFO 09-13 20:56:10 model_runner.py:1300] Graph capturing finished in 22 secs.
[1;36m(VllmWorkerProcess pid=380464)[0;0m INFO 09-13 20:56:10 model_runner.py:1300] Graph capturing finished in 22 secs.
[1;36m(VllmWorkerProcess pid=380467)[0;0m INFO 09-13 20:56:10 model_runner.py:1300] Graph capturing finished in 22 secs.
[1;36m(VllmWorkerProcess pid=380463)[0;0m INFO 09-13 20:56:10 model_runner.py:1300] Graph capturing finished in 22 secs.
INFO 09-13 20:56:10 block_manager_v1.py:263] Automatic prefix caching is enabled.
Control task: {args.control_tasks}
Pre-query template: <s> [INST] system [/INST] 

You are an AI assistant designed to provide helpful, step-by-step guidance on coding problems. The user will ask you a wide range of coding questions.
Your purpose is to assist users in understanding coding concepts, working through code, and arriving at the correct solutions. 日本語で応答してください。<INST>user</INST>


Stop tokens: ['<s>', '</s>', '[INST]', '[/INST]', 'assistant', '\n']
Stop token ids: [1, 2, 3, 4]
Checkpoint saved. Total prompts: 200
Instruction generated from mistralai/Mixtral-8x22B-Instruct-v0.1. Total prompts: 200
ERROR 09-13 20:56:35 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 380465 died, exit code: -15
INFO 09-13 20:56:35 multiproc_worker_utils.py:123] Killing local vLLM worker processes
