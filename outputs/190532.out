Instruction Generation Manager. Arguments: Namespace(model_path='mistralai/Mixtral-8x22B-Instruct-v0.1', temperature=1.0, top_p=1.0, n=200, repeat=None, total_prompts=50, max_tokens=2048, max_model_len=4096, early_stopping=True, system_prompt=False, sanitize=False, logits_processor=False, control_tasks='code', shuffle=True, skip_special_tokens=True, checkpoint_every=100, engine='vllm', device='0', dtype='bfloat16', tensor_parallel_size=1, gpu_memory_utilization=0.95, swap_space=2.0, output_folder='data', job_name=None, timestamp=1726212073, seed=None)
INFO 09-13 16:21:14 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='mistralai/Mixtral-8x22B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mixtral-8x22B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1726212073, served_model_name=mistralai/Mixtral-8x22B-Instruct-v0.1, use_v2_block_manager=False, enable_prefix_caching=True)
INFO 09-13 16:21:16 model_runner.py:879] Starting to load model mistralai/Mixtral-8x22B-Instruct-v0.1...
